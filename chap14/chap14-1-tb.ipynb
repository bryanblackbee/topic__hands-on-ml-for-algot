{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T04:13:02.625555Z",
     "start_time": "2020-11-05T04:13:02.610870Z"
    }
   },
   "source": [
    "# 14. Working with Text Data\n",
    "\n",
    "## How to extract features from text data\n",
    "\n",
    "### Challenges of NLP\n",
    "The reason why NLP is complex is because of\n",
    "- ambiguity due to polysemy (a word or phrase can have different meanings depending on contexts)\n",
    "- non-standard and evolving use of language\n",
    "- the use of idioms\n",
    "- presence of named entities\n",
    "- knowledge of the world\n",
    "\n",
    "### The NLP Workflow\n",
    "The general NLP workflow includes:\n",
    "1. Parsing & Tokenising text\n",
    "2. Linguistic annotation\n",
    "3. Semantic annotation\n",
    "4. Document modelling\n",
    "5. Document labelling (labelling the y-variable or outcome variable)\n",
    "6. Data enrichment\n",
    "7. Predictive modelling\n",
    "\n",
    "Collections of text data exist as a **corpus**. Each corpus contains many files, websites or other artifects called **documents**.\n",
    "\n",
    "The fundamental techniques of extracting text features from a corpus involve breaking down a document into **tokens**. The **bag of words (BoW)** model uses token frequency to construct a document-term matrix (or, transposed a term-document matrix) used for modelling.\n",
    "\n",
    "In contrast, advanced models used on text to extract features include topic models that reflect the joint usage of tokens, and word-vector models that capture the context of word usage.\n",
    "\n",
    "In the feature extraction step, some key techniques used include:\n",
    "\n",
    "|Feature|Description|\n",
    "|--|--|\n",
    "| Tokenisation | breaking a text into n-grams to discover vocabulary|\n",
    "|POS (Part-of-speech) Tagging | to identify a function of a token usingsyntactic & grammatical rules\n",
    "| Dependency Parsing | to identify hierarchical relationships amongsttokens\n",
    "| Stemming & Lemmatisation | to find root forms of a word|\n",
    "Sentence Boundary Detection |to find and segment text intoindividual sentences\n",
    "| Named Entity Recognition | to obtain names of objects of interest,and this can feed into a knowledge graph\n",
    "| Similarity Scoring | To find texts referring to the same subject /contain similar sentiment\n",
    "\n",
    "### Use Cases\n",
    "1. Sentiment analysis of new products to identify competitive position of a company\n",
    "2. Anomaly detection to predict probability of default\n",
    "3. Prediction of news impacting a stock price\n",
    "\n",
    "## From text to tokens - the NLP Pipeline\n",
    "\n",
    "### Text to Tokens with spaCy and textacy\n",
    "(TODO) Refer to Notebook A. Competencies covered include:\n",
    "1. Tokenising and annotating a sentence using spaCy\n",
    "\n",
    "## From tokens to numbers - the document-term matrix\n",
    "\n",
    "### The BoW model\n",
    "The BoW model represents a document using term frequencies. Each document is a vector for each element in the vector representing a word in the **vocabulary**.\n",
    "\n",
    "Intuitively, the BoW model is a simplified language model as it only considers token frequency without considering order & grammatical relationships. However, it achieves good results and is often used as a baseline model for machine learning problems.\n",
    "\n",
    "### Measuring similarity of documents\n",
    "Often, document similarity is calculated using the cosine similarity of two document vectors. Given a document vector and its query vector, a high cosine similarity (close to 1) indicate high similarity while a low value (close to 0) indicate low similarity.\n",
    "\n",
    "\n",
    "### Document-term matrix with sklearn\n",
    "An implementation of converting a corpus to a document-term matrix is by using `sklearn`'s `CountVectorizer` implementation.\n",
    "\n",
    "A more advanced way of constructing the document-term matrix is the `TfidfVectorizer`, which uses the TF-IDF or term-frequency inverse-document frequency method. TF-IDF penalises words that appear in many documents, compared to the same words that appear in fewer documents.\n",
    "\n",
    "To fine-tune the document term matrix, a few strategies are listed below:\n",
    "- Stop word removal\n",
    "- tuning the ngram range\n",
    "- tune to ignore case (upper/lower case)\n",
    "- tune to ignore words outside a [min, max] document frequency range\n",
    "- tune to only pick the [max features] number of tokens in the voacbulary\n",
    "\n",
    "(TODO) Refer to Notebook B. Competencies covered include:\n",
    "1. Converting a corpus to a document-term matrix using `CountVectorizer` & `TfidfVectorizer`\n",
    "2. Find similar documents using `scipy.spatial.distance`\n",
    "\n",
    "## Text classification and sentiment analysis\n",
    "\n",
    "### Text Classification\n",
    "Common applications for text classification include:\n",
    "- spam/non-spam\n",
    "- sentiment analysis\n",
    "\n",
    "### Models Covered under classification\n",
    "- Naive Bayes Classification\n",
    "  - Binomial Model\n",
    "  - Multinomial Model\n",
    "- Logistic Regression\n",
    "  - one-versus-all version for multiclass classification\n",
    "  - multinomial model\n",
    "- Gradient Boosted Trees\n",
    "  - LightGBM implementation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
