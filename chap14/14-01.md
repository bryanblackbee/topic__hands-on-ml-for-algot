# 14. Working with Text Data

## How to extract features from text data

### Challenges of NLP
The reason why NLP is complex is because of
- ambiguity due to polysemy (a word or phrase can have different meanings depending on contexts)
- non-standard and evolving use of language
- the use of idioms
- presence of named entities
- knowledge of the world

### The NLP Workflow
The general NLP workflow includes:
1. Parsing & Tokenising text
2. Linguistic annotation
3. Semantic annotation
4. Document modelling
5. Document labelling (labelling the y-variable or outcome variable)
6. Data enrichment
7. Predictive modelling

Collections of text data exist as a **corpus**. Each corpus contains many files, websites or other artifects called **documents**.

The fundamental techniques of extracting text features from a corpus involve breaking down a document into **tokens**. The **bag of words (BoW)** model uses token frequency to construct a document-term matrix (or, transposed a term-document matrix) used for modelling.

In contrast, advanced models used on text to extract features include topic models that reflect the joint usage of tokens, and word-vector models that capture the context of word usage.

In the feature extraction step, some key techniques used include:

|Feature|Description|
|--|--|
| Tokenisation | breaking a text into n-grams to discover vocabulary|
|POS (Part-of-speech) Tagging | to identify a function of a token usingsyntactic & grammatical rules
| Dependency Parsing | to identify hierarchical relationships amongsttokens
| Stemming & Lemmatisation | to find root forms of a word|
Sentence Boundary Detection |to find and segment text intoindividual sentences
| Named Entity Recognition | to obtain names of objects of interest,and this can feed into a knowledge graph
| Similarity Scoring | To find texts referring to the same subject /contain similar sentiment

### Use Cases
1. Sentiment analysis of new products to identify competitive position of a company
2. Anomaly detection to predict probability of default
3. Prediction of news impacting a stock price

## From text to tokens - the NLP Pipeline

### Text to Tokens with spaCy and textacy
(TODO) Refer to Notebook A. Competencies covered include:
1. Tokenising and annotating a sentence using spaCy

## From tokens to numbers - the document-term matrix

### The BoW model
The BoW model represents a document using term frequencies. Each document is a vector for each element in the vector representing a word in the **vocabulary**.

Intuitively, the BoW model is a simplified language model as it only considers token frequency without considering order & grammatical relationships. However, it achieves good results and is often used as a baseline model for machine learning problems.

### Measuring similarity of documents
Often, document similarity is calculated using the cosine similarity of two document vectors. Given a document vector and its query vector, a high cosine similarity (close to 1) indicate high similarity while a low value (close to 0) indicate low similarity.


### Document-term matrix with sklearn
An implementation of converting a corpus to a document-term matrix is by using `sklearn`'s `CountVectorizer` implementation.

A more advanced way of constructing the document-term matrix is the `TfidfVectorizer`, which uses the TF-IDF or term-frequency inverse-document frequency method. TF-IDF penalises words that appear in many documents, compared to the same words that appear in fewer documents.

To fine-tune the document term matrix, a few strategies are listed below:
- Stop word removal
- tuning the ngram range
- tune to ignore case (upper/lower case)
- tune to ignore words outside a [min, max] document frequency range
- tune to only pick the [max features] number of tokens in the voacbulary

(TODO) Refer to Notebook B. Competencies covered include:
1. Converting a corpus to a document-term matrix using `CountVectorizer` & `TfidfVectorizer`
2. Find similar documents using `scipy.spatial.distance`

## Text classification and sentiment analysis

### Text Classification
Common applications for text classification include:
- spam/non-spam
- sentiment analysis

### Models Covered under classification
- Naive Bayes Classification
  - Binomial Model
  - Multinomial Model
- Logistic Regression
  - one-versus-all version for multiclass classification
  - multinomial model
- Gradient Boosted Trees
  - LightGBM implementation
